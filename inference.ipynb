{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15c19a6",
   "metadata": {},
   "source": [
    "### 1. Prepare the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77de1ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 5884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Observation Grid:\\n[0,0]: depth=0.00, sem=void; [0,1]: depth=0.00, sem=void; [0,2]: depth=0.00, sem=void; [0,3]: depth=0.00, sem=void; [0,4]: depth=0.00, sem=void; [0,5]: depth=0.00, sem=void\\n[1,0]: depth=0.00, sem=void; [1,1]: depth=0.00, sem=void; [1,2]: depth=0.00, sem=void; [1,3]: depth=0.00, sem=void; [1,4]: depth=0.00, sem=void; [1,5]: depth=0.00, sem=void\\n[2,0]: depth=2.37, sem=void; [2,1]: depth=0.09, sem=void; [2,2]: depth=0.00, sem=void; [2,3]: depth=0.00, sem=void; [2,4]: depth=0.00, sem=void; [2,5]: depth=0.00, sem=void\\n[3,0]: depth=4.81, sem=wall; [3,1]: depth=0.28, sem=void; [3,2]: depth=0.00, sem=void; [3,3]: depth=0.00, sem=void; [3,4]: depth=0.00, sem=void; [3,5]: depth=0.00, sem=void\\n[4,0]: depth=2.38, sem=void; [4,1]: depth=0.05, sem=void; [4,2]: depth=0.00, sem=void; [4,3]: depth=0.00, sem=void; [4,4]: depth=0.00, sem=void; [4,5]: depth=0.00, sem=void\\n[5,0]: depth=0.00, sem=void; [5,1]: depth=0.00, sem=void; [5,2]: depth=0.00, sem=void; [5,3]: depth=0.00, sem=void; [5,4]: depth=0.00, sem=void; [5,5]: depth=0.00, sem=void\\nInstruction: Turn to the left.  Go straight, through the door, up the stairs.  When you get upstairs, go straight to the door.  This leads you to the outside.  Go just to the doorway and then stop.',\n",
       " 'action': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from l2am.dataset_utils import prepare_text_samples_batch\n",
    "from datasets import load_dataset, Dataset\n",
    "data_path = \"data/l2am_r2r/episodes_part_0008.json\"\n",
    "raw_ds = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "# 应用转换\n",
    "frame_ds = raw_ds.map(\n",
    "        prepare_text_samples_batch,\n",
    "        batched=True,\n",
    "        remove_columns=raw_ds.column_names,\n",
    "        desc=\"Building text prompts\",\n",
    "        num_proc=4  # 并行加速（可选）\n",
    "    )\n",
    "print(f\"Total frames: {len(frame_ds)}\")\n",
    "frame_ds[0]  # 查看一个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c94babf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "------------------------------>\n",
      "Observation Grid:\n",
      "[0,0]: depth=0.00, sem=void; [0,1]: depth=0.00, sem=void; [0,2]: depth=0.00, sem=void; [0,3]: depth=0.00, sem=void; [0,4]: depth=0.00, sem=void; [0,5]: depth=0.00, sem=void\n",
      "[1,0]: depth=0.00, sem=void; [1,1]: depth=0.00, sem=void; [1,2]: depth=0.00, sem=void; [1,3]: depth=0.00, sem=void; [1,4]: depth=0.00, sem=void; [1,5]: depth=0.00, sem=void\n",
      "[2,0]: depth=2.37, sem=void; [2,1]: depth=0.09, sem=void; [2,2]: depth=0.00, sem=void; [2,3]: depth=0.00, sem=void; [2,4]: depth=0.00, sem=void; [2,5]: depth=0.00, sem=void\n",
      "[3,0]: depth=4.81, sem=wall; [3,1]: depth=0.28, sem=void; [3,2]: depth=0.00, sem=void; [3,3]: depth=0.00, sem=void; [3,4]: depth=0.00, sem=void; [3,5]: depth=0.00, sem=void\n",
      "[4,0]: depth=2.38, sem=void; [4,1]: depth=0.05, sem=void; [4,2]: depth=0.00, sem=void; [4,3]: depth=0.00, sem=void; [4,4]: depth=0.00, sem=void; [4,5]: depth=0.00, sem=void\n",
      "[5,0]: depth=0.00, sem=void; [5,1]: depth=0.00, sem=void; [5,2]: depth=0.00, sem=void; [5,3]: depth=0.00, sem=void; [5,4]: depth=0.00, sem=void; [5,5]: depth=0.00, sem=void\n",
      "Instruction: Turn to the left.  Go straight, through the door, up the stairs.  When you get upstairs, go straight to the door.  This leads you to the outside.  Go just to the doorway and then stop.\n",
      "++++++++++++++++++++++++++++++\n",
      "Ground Truth Action:\n",
      "------------------------------>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt:\")\n",
    "print(\"------------------------------>\")\n",
    "print(frame_ds[0]['prompt'])  # 查看对应的文本提示\n",
    "print(\"+\" * 30)\n",
    "print(\"Ground Truth Action:\")  # 查看对应的动作标签\n",
    "print(\"------------------------------>\")\n",
    "print(frame_ds[0]['action'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb798303",
   "metadata": {},
   "source": [
    "### 2. Inference example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5081e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = frame_ds[0]['prompt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd83790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 5884\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Prompt:\n",
      "Observation Grid:\n",
      "[0,0]: depth=1.71, sem=wall; [0,1]: depth=0.75, sem=window; [0,2]: depth=0.00, sem=void; [0,3]: depth=0.00, sem=void; [0,4]: depth=0.00, sem=void; [0,5]: depth=0.00, sem=void\n",
      "[1,0]: depth=1.63, sem=wall; [1,1]: depth=0.74, sem=window; [1,2]: depth=0.00, sem=void; [1,3]: depth=0.00, sem=void; [1,4]: depth=0.00, sem=void; [1,5]: depth=0.00, sem=void\n",
      "[2,0]: depth=1.62, sem=wall; [2,1]: depth=1.07, sem=window; [2,2]: depth=3.68, sem=void; [2,3]: depth=1.50, sem=void; [2,4]: depth=0.00, sem=void; [2,5]: depth=0.00, sem=void\n",
      "[3,0]: depth=1.60, sem=wall; [3,1]: depth=1.25, sem=window; [3,2]: depth=6.75, sem=window; [3,3]: depth=3.86, sem=wall; [3,4]: depth=0.00, sem=void; [3,5]: depth=0.00, sem=void\n",
      "[4,0]: depth=1.34, sem=floor; [4,1]: depth=1.08, sem=window; [4,2]: depth=1.23, sem=void; [4,3]: depth=0.70, sem=void; [4,4]: depth=0.00, sem=void; [4,5]: depth=0.00, sem=void\n",
      "[5,0]: depth=0.82, sem=floor; [5,1]: depth=0.75, sem=window; [5,2]: depth=0.82, sem=void; [5,3]: depth=0.27, sem=void; [5,4]: depth=0.00, sem=void; [5,5]: depth=0.00, sem=void\n",
      "Instruction: Turn to the left.  Go straight, through the door, up the stairs.  When you get upstairs, go straight to the door.  This leads you to the outside.  Go just to the doorway and then stop.\n",
      "\n",
      "Ground Truth Action: 2\n",
      "Predicted Action: 2\n",
      "==================================================\n",
      "✅ Prediction matches ground truth!\n"
     ]
    }
   ],
   "source": [
    "# inference.py\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from l2am.model_zoo import WeightedSequenceClassifier  # 确保能导入\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# ======================\n",
    "# 配置\n",
    "# ======================\n",
    "MODEL_CHECKPOINT = \"data/l2a_longformer_action_classifier/checkpoint-81000\"\n",
    "HF_CACHE_DIR = \"data/hf_model_cache\"\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "# 从 dataset 准备一个样本进行测试，构建prompt示例代码如下：\n",
    "from l2am.dataset_utils import prepare_text_samples_batch\n",
    "from datasets import load_dataset, Dataset\n",
    "data_path = \"data/l2am_r2r/episodes_part_0008.json\"\n",
    "raw_ds = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "# 应用转换\n",
    "frame_ds = raw_ds.map(\n",
    "        prepare_text_samples_batch,\n",
    "        batched=True,\n",
    "        remove_columns=raw_ds.column_names,\n",
    "        desc=\"Building text prompts\",\n",
    "        num_proc=4  # 并行加速（可选）\n",
    "    )\n",
    "print(f\"Total frames: {len(frame_ds)}\")\n",
    "\n",
    "test_frame_id = 3  # 选择要测试的样本 ID\n",
    "EXAMPLE_PROMPT = frame_ds[test_frame_id]['prompt']\n",
    "\n",
    "GROUND_TRUTH_ACTION = frame_ds[test_frame_id]['action']\n",
    "\n",
    "\n",
    "\n",
    "def main(hf_cache_dir=HF_CACHE_DIR, model_checkpoint=MODEL_CHECKPOINT, max_length=MAX_LENGTH):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Step 1: 加载 tokenizer（从 checkpoint 目录或原始模型）\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        cache_dir=hf_cache_dir,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    # Step 2: 推断 num_labels（根据你的任务，已知是 4 类？）\n",
    "    num_labels = 4  # class_0 ~ class_3 \n",
    "\n",
    "    # Step 3: 重建模型结构（必须与训练时完全一致）\n",
    "    # 注意：class_weights 在推理时不影响前向传播（只用于 loss），所以可以传 dummy 值\n",
    "    dummy_class_weights = torch.ones(num_labels)  # 推理时 loss 不计算，权重无影响\n",
    "\n",
    "    model = WeightedSequenceClassifier(\n",
    "        model_name=\"allenai/longformer-base-4096\",  # 或从 checkpoint 加载 config\n",
    "        num_labels=num_labels,\n",
    "        class_weights=dummy_class_weights,\n",
    "        cache_dir=hf_cache_dir,\n",
    "    )\n",
    "\n",
    "    # Step 4: 加载训练好的权重\n",
    "    \n",
    "    model_file = os.path.join(model_checkpoint, \"model.safetensors\")\n",
    "    state_dict = load_file(model_file, device=str(device))\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Step 5: Tokenize & 推理\n",
    "    inputs = tokenizer(\n",
    "        EXAMPLE_PROMPT,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # logits = outputs.logits\n",
    "        logits = outputs[\"logits\"]\n",
    "        pred_class = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Prompt:\")\n",
    "    print(EXAMPLE_PROMPT)\n",
    "    print(\"\\nGround Truth Action:\", GROUND_TRUTH_ACTION)\n",
    "    print(\"Predicted Action:\", pred_class)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if pred_class == GROUND_TRUTH_ACTION:\n",
    "        print(\"✅ Prediction matches ground truth!\")\n",
    "    else:\n",
    "        print(\"❌ Prediction differs from ground truth.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb3059",
   "metadata": {},
   "source": [
    "### 3. Action chunk inference with bigbird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d473f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 5884\n",
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "Prompt:\n",
      "Observation Grid:\n",
      "[0,0]: depth=1.71, sem=wall; [0,1]: depth=0.75, sem=window; [0,2]: depth=0.00, sem=void; [0,3]: depth=0.00, sem=void; [0,4]: depth=0.00, sem=void; [0,5]: depth=0.00, sem=void\n",
      "[1,0]: depth=1.63, sem=wall; [1,1]: depth=0.74, sem=window; [1,2]: depth=0.00, sem=void; [1,3]: depth=0.00, sem=void; [1,4]: depth=0.00, sem=void; [1,5]: depth=0.00, sem=void\n",
      "[2,0]: depth=1.62, sem=wall; [2,1]: depth=1.07, sem=window; [2,2]: depth=3.68, sem=void; [2,3]: depth=1.50, sem=void; [2,4]: depth=0.00, sem=void; [2,5]: depth=0.00, sem=void\n",
      "[3,0]: depth=1.60, sem=wall; [3,1]: depth=1.25, sem=window; [3,2]: depth=6.75, sem=window; [3,3]: depth=3.86, sem=wall; [3,4]: depth=0.00, sem=void; [3,5]: depth=0.00, sem=void\n",
      "[4,0]: depth=1.34, sem=floor; [4,1]: depth=1.08, sem=window; [4,2]: depth=1.23, sem=void; [4,3]: depth=0.70, sem=void; [4,4]: depth=0.00, sem=void; [4,5]: depth=0.00, sem=void\n",
      "[5,0]: depth=0.82, sem=floor; [5,1]: depth=0.75, sem=window; [5,2]: depth=0.82, sem=void; [5,3]: depth=0.27, sem=void; [5,4]: depth=0.00, sem=void; [5,5]: depth=0.00, sem=void\n",
      "Instruction: Turn to the left.  Go straight, through the door, up the stairs.  When you get upstairs, go straight to the door.  This leads you to the outside.  Go just to the doorway and then stop.\n",
      "\n",
      "Ground Truth Action: [2, 2, 2, 2]\n",
      "Predicted Action: [2, 2, 2, 2]\n",
      "==================================================\n",
      "✅ Prediction matches ground truth!\n"
     ]
    }
   ],
   "source": [
    "# inference.py\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from l2am.model_zoo import MultiStepWeightedClassifier  # 确保能导入\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# ======================\n",
    "# 配置\n",
    "# ======================\n",
    "MODEL_CHECKPOINT = \"data/l2a_bigbird_action_classifier_chunk4/checkpoint-185500\"\n",
    "HF_CACHE_DIR = \"data/hf_model_cache\"\n",
    "MAX_LENGTH = 1024\n",
    "MODEL_NAME = \"google/bigbird-roberta-base\"  # 可替换为 roberta-base、 bert-base-uncased、allenai/longformer-base-4096、google/bigbird-roberta-base等\n",
    "\n",
    "NUM_CHUNK = 4  # 与训练时保持一致\n",
    "\n",
    "# 从 dataset 准备一个样本进行测试，构建prompt示例代码如下：\n",
    "from l2am.dataset_utils import prepare_text_samples_batch_chunk_v1\n",
    "from datasets import load_dataset, Dataset\n",
    "data_path = \"data/l2am_r2r/episodes_part_0008.json\"\n",
    "raw_ds = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "# 应用转换\n",
    "frame_ds = raw_ds.map(\n",
    "        prepare_text_samples_batch_chunk_v1,\n",
    "        batched=True,\n",
    "        remove_columns=raw_ds.column_names,\n",
    "        desc=\"Building text prompts\",\n",
    "        num_proc=16  # 并行加速（可选）\n",
    "    )\n",
    "print(f\"Total frames: {len(frame_ds)}\")\n",
    "\n",
    "test_frame_id = 3  # 选择要测试的样本 ID\n",
    "EXAMPLE_PROMPT = frame_ds[test_frame_id]['prompt']\n",
    "\n",
    "GROUND_TRUTH_ACTION = frame_ds[test_frame_id]['action_chunk']\n",
    "\n",
    "\n",
    "\n",
    "def main(hf_cache_dir=HF_CACHE_DIR, model_checkpoint=MODEL_CHECKPOINT, max_length=MAX_LENGTH):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Step 1: 加载 tokenizer（从 checkpoint 目录或原始模型）\n",
    "    from transformers import BigBirdTokenizer\n",
    "\n",
    "    tokenizer = BigBirdTokenizer.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        cache_dir=hf_cache_dir,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )\n",
    "\n",
    "    # Step 2: 推断 num_labels（根据你的任务，已知是 4 类？）\n",
    "    num_labels = 4  # class_0 ~ class_3 \n",
    "\n",
    "    # Step 3: 重建模型结构（必须与训练时完全一致）\n",
    "    # 注意：class_weights 在推理时不影响前向传播（只用于 loss），所以可以传 dummy 值\n",
    "    dummy_class_weights = torch.ones(num_labels)  # 推理时 loss 不计算，权重无影响\n",
    "\n",
    "    \n",
    "    model = MultiStepWeightedClassifier(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        class_weights=dummy_class_weights,\n",
    "        num_steps=NUM_CHUNK,\n",
    "        cache_dir=hf_cache_dir,\n",
    "    )\n",
    "\n",
    "    # Step 4: 加载训练好的权重\n",
    "    model_safetensors = os.path.join(model_checkpoint, \"model.safetensors\")\n",
    "    model_bin = os.path.join(model_checkpoint, \"pytorch_model.bin\")\n",
    "\n",
    "    if os.path.exists(model_safetensors):\n",
    "        state_dict = load_file(model_safetensors, device=str(device))\n",
    "    elif os.path.exists(model_bin):\n",
    "        state_dict = torch.load(model_bin, map_location=device)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Neither 'model.safetensors' nor 'pytorch_model.bin' found in checkpoint directory.\")\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Step 5: Tokenize & 推理\n",
    "    inputs = tokenizer(\n",
    "        EXAMPLE_PROMPT,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[\"logits\"]\n",
    "        # 对每个 step 取 argmax -> (B, num_steps)\n",
    "        pred_classes = torch.argmax(logits, dim=-1)  # 注意：不是 .item()\n",
    "        # pred_class = torch.argmax(logits, dim=-1).item()\n",
    "         # 如果 batch_size == 1，可以 squeeze 得到 (num_steps,)\n",
    "        if pred_classes.shape[0] == 1:\n",
    "            pred_classes = pred_classes.squeeze(0)  # shape: (num_steps,)\n",
    "\n",
    "        # 转为 Python list（便于打印或比较）\n",
    "        pred_list = pred_classes.cpu().tolist()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Prompt:\")\n",
    "    print(EXAMPLE_PROMPT)\n",
    "    print(\"\\nGround Truth Action:\", GROUND_TRUTH_ACTION)\n",
    "    print(\"Predicted Action:\", pred_list)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if pred_list == GROUND_TRUTH_ACTION:\n",
    "        print(\"✅ Prediction matches ground truth!\")\n",
    "    else:\n",
    "        print(\"❌ Prediction differs from ground truth.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3c69e",
   "metadata": {},
   "source": [
    "### 4. Action chunk inference with bigbird (using class api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0b1453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 1110\n",
      "[ActionChunkPredictor] Using device: cuda\n",
      "Loading weights from pytorch_model.bin: data/l2a_bigbird_action_classifier_chunk4/checkpoint-189500/pytorch_model.bin\n",
      "\n",
      "==================================================\n",
      "Prompt:\n",
      "Observation Grid:\n",
      "[0,0]: depth=2.06, sem=ceiling; [0,1]: depth=2.06, sem=ceiling; [0,2]: depth=2.06, sem=ceiling; [0,3]: depth=1.65, sem=ceiling; [0,4]: depth=0.94, sem=column; [0,5]: depth=0.94, sem=wall\n",
      "[1,0]: depth=2.63, sem=wall; [1,1]: depth=2.91, sem=wall; [1,2]: depth=2.96, sem=wall; [1,3]: depth=2.38, sem=column; [1,4]: depth=0.94, sem=column; [1,5]: depth=0.84, sem=wall\n",
      "[2,0]: depth=3.59, sem=wall; [2,1]: depth=5.57, sem=wall; [2,2]: depth=4.36, sem=wall; [2,3]: depth=3.95, sem=wall; [2,4]: depth=0.94, sem=column; [2,5]: depth=0.95, sem=wall\n",
      "[3,0]: depth=2.90, sem=wall; [3,1]: depth=3.46, sem=floor; [3,2]: depth=3.16, sem=floor; [3,3]: depth=2.88, sem=column; [3,4]: depth=0.95, sem=column; [3,5]: depth=0.95, sem=wall\n",
      "[4,0]: depth=1.38, sem=floor; [4,1]: depth=1.38, sem=floor; [4,2]: depth=1.37, sem=floor; [4,3]: depth=1.24, sem=floor; [4,4]: depth=0.93, sem=column; [4,5]: depth=0.86, sem=wall\n",
      "[5,0]: depth=0.80, sem=wall; [5,1]: depth=0.80, sem=wall; [5,2]: depth=0.80, sem=wall; [5,3]: depth=0.80, sem=wall; [5,4]: depth=0.77, sem=floor; [5,5]: depth=0.62, sem=stairs\n",
      "Instruction: Head toward the fireplace and turn right. Stop in the hallway near the elevator.\n",
      "\n",
      "Ground Truth Action: [1, 1, 1, 1]\n",
      "Predicted Action:      [1, 1, 1, 1]\n",
      "Predicted Action (Clean):      [1, 1, 1, 1]\n",
      "==================================================\n",
      "✅ Prediction matches ground truth!\n"
     ]
    }
   ],
   "source": [
    "from l2am.inference_api import ActionChunkPredictor\n",
    "from l2am.dataset_utils import prepare_text_samples_batch_chunk_v1\n",
    "from datasets import load_dataset\n",
    "\n",
    "def main():\n",
    "    # ======================\n",
    "    # 配置\n",
    "    # ======================\n",
    "    MODEL_CHECKPOINT = \"data/l2a_bigbird_action_classifier_chunk4/checkpoint-189500\"\n",
    "    HF_CACHE_DIR = \"data/hf_model_cache\"\n",
    "    MAX_LENGTH = 1024\n",
    "    NUM_CHUNK = 4\n",
    "    NUM_LABELS = 4\n",
    "\n",
    "    # 加载测试样本\n",
    "    data_path = \"data/l2am_r2r/episodes_part_0109.json\"\n",
    "    raw_ds = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "    frame_ds = raw_ds.map(\n",
    "        prepare_text_samples_batch_chunk_v1,\n",
    "        batched=True,\n",
    "        remove_columns=raw_ds.column_names,\n",
    "        desc=\"Building text prompts\",\n",
    "        num_proc=16,\n",
    "    )\n",
    "    print(f\"Total frames: {len(frame_ds)}\")\n",
    "\n",
    "    # 初始化预测器\n",
    "    predictor = ActionChunkPredictor(\n",
    "        model_checkpoint=MODEL_CHECKPOINT,\n",
    "        hf_cache_dir=HF_CACHE_DIR,\n",
    "        num_labels=NUM_LABELS,\n",
    "        num_steps=NUM_CHUNK,\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    \n",
    "    test_frame_id = 35\n",
    "    EXAMPLE_PROMPT = frame_ds[test_frame_id]['prompt']\n",
    "    GROUND_TRUTH_ACTION = frame_ds[test_frame_id]['action_chunk']\n",
    "\n",
    "    # 推理\n",
    "    pred_action = predictor.predict(EXAMPLE_PROMPT)\n",
    "    pred_action_clean = predictor.predict_clean(EXAMPLE_PROMPT) # 自动处理action chunk中的特殊补全标记\n",
    "\n",
    "    # 输出结果\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Prompt:\")\n",
    "    print(EXAMPLE_PROMPT)\n",
    "    print(\"\\nGround Truth Action:\", GROUND_TRUTH_ACTION)\n",
    "    print(\"Predicted Action:     \", pred_action)\n",
    "    print(\"Predicted Action (Clean):     \", pred_action_clean)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if pred_action == GROUND_TRUTH_ACTION:\n",
    "        print(\"✅ Prediction matches ground truth!\")\n",
    "    else:\n",
    "        print(\"❌ Prediction differs from ground truth.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ecd502",
   "metadata": {},
   "source": [
    "#### 4.1 Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a95d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 1110\n",
      "[ActionChunkPredictor] Using device: cuda\n",
      "Loading weights from pytorch_model.bin: data/l2a_bigbird_action_classifier_chunk4/checkpoint-189500/pytorch_model.bin\n",
      "\n",
      "==================================================\n",
      "Prompt:\n",
      "Observation Grid:\n",
      "[0,0]: depth=2.64, sem=wall; [0,1]: depth=2.53, sem=beam; [0,2]: depth=2.41, sem=beam; [0,3]: depth=1.93, sem=column; [0,4]: depth=0.32, sem=railing; [0,5]: depth=0.28, sem=railing\n",
      "[1,0]: depth=3.75, sem=ceiling; [1,1]: depth=3.28, sem=column; [1,2]: depth=3.57, sem=ceiling; [1,3]: depth=2.07, sem=objects; [1,4]: depth=0.39, sem=railing; [1,5]: depth=0.28, sem=railing\n",
      "[2,0]: depth=2.78, sem=chair; [2,1]: depth=3.58, sem=wall; [2,2]: depth=6.32, sem=wall; [2,3]: depth=3.55, sem=column; [2,4]: depth=0.50, sem=railing; [2,5]: depth=0.28, sem=railing\n",
      "[3,0]: depth=1.88, sem=chair; [3,1]: depth=2.75, sem=wall; [3,2]: depth=3.48, sem=wall; [3,3]: depth=2.56, sem=column; [3,4]: depth=0.50, sem=railing; [3,5]: depth=0.24, sem=railing\n",
      "[4,0]: depth=1.07, sem=wall; [4,1]: depth=1.07, sem=wall; [4,2]: depth=1.06, sem=wall; [4,3]: depth=0.97, sem=wall; [4,4]: depth=0.44, sem=railing; [4,5]: depth=0.20, sem=railing\n",
      "[5,0]: depth=0.63, sem=wall; [5,1]: depth=0.63, sem=wall; [5,2]: depth=0.62, sem=wall; [5,3]: depth=0.62, sem=wall; [5,4]: depth=0.41, sem=railing; [5,5]: depth=0.21, sem=railing\n",
      "Instruction: Head toward the fireplace and turn right. Stop in the hallway near the elevator.\n",
      "\n",
      "Ground Truth Action: [2, 1, 1, 1]\n",
      "Predicted Action:      [2, 1, 1, 1]\n",
      "Predicted Action (Clean):      [2, 1, 1, 1]\n",
      "==================================================\n",
      "✅ Prediction matches ground truth!\n"
     ]
    }
   ],
   "source": [
    "from l2am.inference_api import ActionChunkPredictor\n",
    "from l2am.dataset_utils import prepare_text_samples_batch_chunk_v1\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "MODEL_CHECKPOINT = \"data/l2a_bigbird_action_classifier_chunk4/checkpoint-189500\"\n",
    "HF_CACHE_DIR = \"data/hf_model_cache\"\n",
    "MAX_LENGTH = 1024\n",
    "NUM_CHUNK = 4\n",
    "NUM_LABELS = 4\n",
    "\n",
    "# 加载测试样本\n",
    "data_path = \"data/l2am_r2r/episodes_part_0109.json\"\n",
    "raw_ds = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "frame_ds = raw_ds.map(\n",
    "    prepare_text_samples_batch_chunk_v1,\n",
    "    batched=True,\n",
    "    remove_columns=raw_ds.column_names,\n",
    "    desc=\"Building text prompts\",\n",
    "    num_proc=16,\n",
    ")\n",
    "print(f\"Total frames: {len(frame_ds)}\")\n",
    "\n",
    "# 初始化预测器\n",
    "predictor = ActionChunkPredictor(\n",
    "    model_checkpoint=MODEL_CHECKPOINT,\n",
    "    hf_cache_dir=HF_CACHE_DIR,\n",
    "    num_labels=NUM_LABELS,\n",
    "    num_steps=NUM_CHUNK,\n",
    "    max_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "# 测试另一个样本\n",
    "test_frame_id = 27\n",
    "EXAMPLE_PROMPT = frame_ds[test_frame_id]['prompt']\n",
    "GROUND_TRUTH_ACTION = frame_ds[test_frame_id]['action_chunk']\n",
    "\n",
    "# 推理\n",
    "pred_action = predictor.predict(EXAMPLE_PROMPT)\n",
    "pred_action_clean = predictor.predict_clean(EXAMPLE_PROMPT) # 自动处理action chunk中的特殊补全标记\n",
    "\n",
    "# 输出结果\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Prompt:\")\n",
    "print(EXAMPLE_PROMPT)\n",
    "print(\"\\nGround Truth Action:\", GROUND_TRUTH_ACTION)\n",
    "print(\"Predicted Action:     \", pred_action)\n",
    "print(\"Predicted Action (Clean):     \", pred_action_clean)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if pred_action == GROUND_TRUTH_ACTION:\n",
    "    print(\"✅ Prediction matches ground truth!\")\n",
    "else:\n",
    "    print(\"❌ Prediction differs from ground truth.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee558bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Prompt:\n",
      "Observation Grid:\n",
      "[0,0]: depth=1.84, sem=wall; [0,1]: depth=1.71, sem=wall; [0,2]: depth=1.74, sem=wall; [0,3]: depth=1.85, sem=wall; [0,4]: depth=1.77, sem=wall; [0,5]: depth=1.34, sem=wall\n",
      "[1,0]: depth=2.08, sem=wall; [1,1]: depth=2.81, sem=ceiling; [1,2]: depth=1.78, sem=wall; [1,3]: depth=3.00, sem=ceiling; [1,4]: depth=2.43, sem=ceiling; [1,5]: depth=1.34, sem=wall\n",
      "[2,0]: depth=2.17, sem=wall; [2,1]: depth=4.82, sem=wall; [2,2]: depth=1.78, sem=wall; [2,3]: depth=4.91, sem=door; [2,4]: depth=3.79, sem=wall; [2,5]: depth=1.34, sem=wall\n",
      "[3,0]: depth=2.16, sem=wall; [3,1]: depth=3.45, sem=floor; [3,2]: depth=1.78, sem=wall; [3,3]: depth=3.27, sem=floor; [3,4]: depth=2.80, sem=wall; [3,5]: depth=1.35, sem=wall\n",
      "[4,0]: depth=1.47, sem=floor; [4,1]: depth=1.46, sem=floor; [4,2]: depth=1.42, sem=floor; [4,3]: depth=1.47, sem=floor; [4,4]: depth=1.44, sem=floor; [4,5]: depth=1.25, sem=wall\n",
      "[5,0]: depth=0.86, sem=floor; [5,1]: depth=0.86, sem=floor; [5,2]: depth=0.86, sem=floor; [5,3]: depth=0.85, sem=floor; [5,4]: depth=0.85, sem=floor; [5,5]: depth=0.85, sem=floor\n",
      "Instruction: Head toward the fireplace and turn right. Stop in the hallway near the elevator.\n",
      "\n",
      "Ground Truth Action: [1, 0, -100, -100]\n",
      "Predicted Action:      [1, 3, 1, 1]\n",
      "Predicted Action (Clean):      [1, 3, 1, 1]\n",
      "==================================================\n",
      "❌ Prediction differs from ground truth.\n"
     ]
    }
   ],
   "source": [
    "# 测试另一个样本\n",
    "test_frame_id = 40\n",
    "EXAMPLE_PROMPT = frame_ds[test_frame_id]['prompt']\n",
    "GROUND_TRUTH_ACTION = frame_ds[test_frame_id]['action_chunk']\n",
    "\n",
    "# 推理\n",
    "pred_action = predictor.predict(EXAMPLE_PROMPT)\n",
    "pred_action_clean = predictor.predict_clean(EXAMPLE_PROMPT) # 自动处理action chunk中的特殊补全标记\n",
    "\n",
    "# 输出结果\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Prompt:\")\n",
    "print(EXAMPLE_PROMPT)\n",
    "print(\"\\nGround Truth Action:\", GROUND_TRUTH_ACTION)\n",
    "print(\"Predicted Action:     \", pred_action)\n",
    "print(\"Predicted Action (Clean):     \", pred_action_clean)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if pred_action == GROUND_TRUTH_ACTION:\n",
    "    print(\"✅ Prediction matches ground truth!\")\n",
    "else:\n",
    "    print(\"❌ Prediction differs from ground truth.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformer_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
