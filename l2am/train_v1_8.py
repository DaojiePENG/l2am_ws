# train.py
import os
# os.environ["NCCL_P2P_DISABLE"] = "1"
# os.environ["NCCL_IB_DISABLE"] = "1"

os.environ["TOKENIZERS_PARALLELISM"] = "false"
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from sklearn.metrics import classification_report, accuracy_score
import warnings
warnings.filterwarnings("ignore", message=".*beta.*renamed.*")
warnings.filterwarnings("ignore", message=".*gamma.*renamed.*")


# ======================
# 1. é…ç½®è·¯å¾„
# ======================
DATA_DIR = "data/l2am_r2r_v3/train/8"
CACHE_DIR = "data/cache/train_frames_v1_8"
VAL_DATA_DIR = "data/l2am_r2r_v3/val_seen/8"
VAL_CACHE_DIR = "data/cache/val_seen_frames_v1_8"
HF_CACHE_DIR = "data/hf_model_cache"  # HF æ¨¡å‹ç¼“å­˜è·¯å¾„
RESUME_FROM_CHECKPOINT = None  # "outputs/l2a_longformer_action_classifier/checkpoint-500"  # è®¾ç½®ä¸ºæŸä¸ªæ£€æŸ¥ç‚¹è·¯å¾„ä»¥ä»è¯¥æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒï¼Œå¦åˆ™ä¸º None
# model configs
MODEL_NAME = "google/bigbird-roberta-base"  # å¯æ›¿æ¢ä¸º roberta-baseã€ bert-base-uncasedã€allenai/longformer-base-4096ã€google/bigbird-roberta-baseç­‰
MAX_LENGTH = 1024  # æ ¹æ®æ¨¡å‹è°ƒæ•´æœ€å¤§é•¿åº¦

# training configs
OUTPUT_DIR = "outputs/l2a_bigbird_action_classifier_v1_8"
NUM_EPOCHS = 30
PER_DEVICE_TRAIN_BATCH_SIZE = 10
PER_DEVICE_EVAL_BATCH_SIZE = 88
GRADIENT_ACCUMULATION_STEPS = 1
LEARNING_RATE = 6e-5
WARMUP_RATIO = 0.02  # å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹
WANDB_RUN_NAME = "bigbird-action-pred-depth-sem_v1_8"
LOGGING_STEPS = 100
EVAL_STEPS = 500
SAVE_STEPS = 500

# ======================
# 2. åŠ è½½æˆ–é¢„å¤„ç†æ•°æ®é›†
# ======================
from dataset_utils import get_or_create_dataset_v1 as get_or_create_dataset


# ======================
# 3. Tokenize å‡½æ•°
# ======================
from dataset_utils import tokenize_function


# ======================
# 4. ä¸»è®­ç»ƒæµç¨‹
# ======================
def main():
    # åŠ è½½åˆ†è¯å™¨
    from transformers import BigBirdTokenizer

    tokenizer = BigBirdTokenizer.from_pretrained(
        MODEL_NAME,
        cache_dir=HF_CACHE_DIR,
        clean_up_tokenization_spaces=True,
    )
    # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,
    #                                           cache_dir=HF_CACHE_DIR,  # â† å’Œ download_model.py ä¸€è‡´
    #                                           clean_up_tokenization_spaces=True,  # ä¿æŒå½“å‰è¡Œä¸ºï¼ˆæ¸…ç†ç©ºæ ¼ï¼‰
    #                                           )

    # Step 1: è·å–å¸§çº§æ•°æ®é›†
    ds = get_or_create_dataset(DATA_DIR, CACHE_DIR)
    vds = get_or_create_dataset(VAL_DATA_DIR, VAL_CACHE_DIR)

    # Step 2: åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    # ds = ds.train_test_split(test_size=0.05, seed=42)
    train_ds = ds
    eval_ds = vds

    # Step 3: Tokenize
    # å¦‚æœæ²¡æœ‰äº‹å…ˆä¿å­˜çš„æ•°æ®é›†ï¼Œåˆ™åˆ›å»ºæ•°æ®é›†
    if os.path.exists(os.path.join(OUTPUT_DIR, "tokenized_train")) and os.path.exists(os.path.join(OUTPUT_DIR, "tokenized_eval")):
        print("Loading tokenized datasets from disk directory:", OUTPUT_DIR)
        from datasets import load_from_disk
        tokenized_train = load_from_disk(os.path.join(OUTPUT_DIR, "tokenized_train"))
        tokenized_eval = load_from_disk(os.path.join(OUTPUT_DIR, "tokenized_eval"))
    else:
        print("Creating and tokenizing datasets...")    
        tokenized_train = train_ds.map(
            lambda x: tokenize_function(x, tokenizer, max_length=MAX_LENGTH),
            batched=True,
            remove_columns=["prompt"],
            num_proc=48,  # ğŸ‘ˆ å…³é”®ï¼ç”¨å¤šè¿›ç¨‹å¹¶è¡Œ tokenize
        )
        tokenized_eval = eval_ds.map(
            lambda x: tokenize_function(x, tokenizer, max_length=MAX_LENGTH),
            batched=True,
            remove_columns=["prompt"],
            num_proc=48,  # ğŸ‘ˆ å…³é”®ï¼ç”¨å¤šè¿›ç¨‹å¹¶è¡Œ tokenize
        )

        tokenized_train = tokenized_train.rename_column("action", "labels")
        tokenized_eval = tokenized_eval.rename_column("action", "labels")

        # ä¿å­˜æ•°æ®é›†ä»¥åè®­ç»ƒæ—¶å¯ç›´æ¥åŠ è½½
        tokenized_train.save_to_disk(os.path.join(OUTPUT_DIR, "tokenized_train"))
        tokenized_eval.save_to_disk(os.path.join(OUTPUT_DIR, "tokenized_eval"))

    # Step 4: ç¡®å®šç±»åˆ«æ•°
    num_labels = len(set(train_ds["action"]))
    print(f"Number of action classes: {num_labels}")

    # Step 5: åŠ è½½æ¨¡å‹
    from model_zoo import WeightedSequenceClassifier
    # è®¡ç®—actionæƒé‡
    from sklearn.utils.class_weight import compute_class_weight
    import numpy as np
    import torch
    labels = np.array(train_ds["action"])
    prompts = np.array(train_ds["prompt"])
    # æ‰“å°ä¸€ä¸ªpromptç¤ºä¾‹ï¼š
    print("Example prompt:", prompts[0])
    class_weights = compute_class_weight(
        class_weight="balanced",
        classes=np.unique(labels),
        y=labels
    )
    # class_weights = torch.tensor(class_weights, dtype=torch.float).to("cuda")  # æˆ– "cpu"
    class_weights = torch.tensor(class_weights, dtype=torch.float) # å¤šå¡è®­ç»ƒæ—¶æ”¾åœ¨ Trainer é‡Œå¤„ç†
    print("Class weights:", class_weights)

    model = WeightedSequenceClassifier(
        MODEL_NAME,
        num_labels=num_labels,
        class_weights=class_weights,
        cache_dir=HF_CACHE_DIR,  # â† å’Œ download_model.py ä¸€è‡´
    )
    
    # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°æ•°é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total params: {total_params / 1e6:.1f}M")
    print(f"Trainable params: {trainable_params / 1e6:.1f}M ({trainable_params == total_params})")

    # Step 6: å®šä¹‰è¯„ä¼°æŒ‡æ ‡
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        preds = np.argmax(logits, axis=1)
        
        # Overall
        acc = accuracy_score(labels, preds)
        
        # Per-class recall (critical for rare class)
        report = classification_report(labels, preds, output_dict=True, zero_division=0)
        metrics = {"accuracy": acc}
        for i in range(4):
            metrics[f"recall_class_{i}"] = report[str(i)]["recall"]
            metrics[f"f1_class_{i}"] = report[str(i)]["f1-score"]
        
        return metrics

    # Step 7: è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,          # â†â†â† å…³é”®ä¿®æ”¹ï¼šè®¾ç½®å­¦ä¹ ç‡
        warmup_ratio=WARMUP_RATIO,                # â†â†â† å…³é”®ä¿®æ”¹ï¼šè®¾ç½®å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹
        weight_decay=0.01,
        logging_steps=LOGGING_STEPS,
        eval_strategy="steps",
        eval_steps=EVAL_STEPS,
        save_strategy="steps",
        save_steps=SAVE_STEPS,
        load_best_model_at_end=True,
        metric_for_best_model="eval_f1_class_0",  # â†â†â† å…³é”®ä¿®æ”¹ï¼šä»¥ç½•è§ç±»F1ä¸ºæœ€ä½³æ¨¡å‹é€‰æ‹©æ ‡å‡†
        greater_is_better=True,
        save_total_limit=2,
        report_to="wandb",                 # â†â†â† å…³é”®ï¼šå¯ç”¨ wandb
        run_name=WANDB_RUN_NAME,    # â† å¯é€‰ï¼šç»™å®éªŒå‘½å
        # report_to="none",
        seed=42,
        dataloader_num_workers=4,
        ddp_find_unused_parameters=True,  # â†â†â† æ·»åŠ è¿™ä¸€è¡Œæ¥æ‰“å¼€ddpçš„unused parameteræ£€æŸ¥
        save_safetensors=False,  # â†â†â† å…³é”®ï¼ç¦ç”¨ safetensorsä»¥é¿å…å…¼å®¹æ€§é—®é¢˜
    )

    # Step 8: æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # Step 9: åˆ›å»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # Step 10: å¼€å§‹è®­ç»ƒ
    print("ğŸš€ Starting training...")
    # ç»§ç»­ä¹‹å‰çš„è®­ç»ƒï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    # trainer.train(resume_from_checkpoint=True)
    if RESUME_FROM_CHECKPOINT is not None:
        print(f"Resuming training from checkpoint: {RESUME_FROM_CHECKPOINT}")
        trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)
    else:
        trainer.train()


    # Step 11: ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_model(os.path.join(OUTPUT_DIR, "final"))
    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, "final"))

    print(f"âœ… Training completed! Model saved to {os.path.join(OUTPUT_DIR, 'final')}")


if __name__ == "__main__":
    main()