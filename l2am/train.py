# train.py
import os
# os.environ["NCCL_P2P_DISABLE"] = "1"
# os.environ["NCCL_IB_DISABLE"] = "1"

os.environ["TOKENIZERS_PARALLELISM"] = "false"
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from sklearn.metrics import classification_report, accuracy_score
import warnings
warnings.filterwarnings("ignore", message=".*beta.*renamed.*")
warnings.filterwarnings("ignore", message=".*gamma.*renamed.*")


# import dataset_utils
# print(">>> dataset_utils file:", dataset_utils.__file__)
# print(">>> Has build_prompt_pos_v2?", hasattr(dataset_utils, 'build_prompt_pos_v2'))
# if hasattr(dataset_utils, 'build_prompt_pos_v2'):
#     import inspect
#     print(inspect.getsource(dataset_utils.build_prompt_pos_v2))

# ======================
# 1. é…ç½®è·¯å¾„
# ======================
DATA_DIR = "data/l2am_r2r"
CACHE_DIR = "data/cache/train_frames"
HF_CACHE_DIR = "data/hf_model_cache"  # HF æ¨¡å‹ç¼“å­˜è·¯å¾„

# model configs
MODEL_NAME = "allenai/longformer-base-4096"  # å¯æ›¿æ¢ä¸º roberta-baseã€ bert-base-uncasedã€allenai/longformer-base-4096ç­‰
MAX_LENGTH = 1024  # æ ¹æ®æ¨¡å‹è°ƒæ•´æœ€å¤§é•¿åº¦

# training configs
OUTPUT_DIR = "outputs/l2a_longformer_action_classifier"
NUM_EPOCHS = 50
PER_DEVICE_TRAIN_BATCH_SIZE = 8
PER_DEVICE_EVAL_BATCH_SIZE = 16
GRADIENT_ACCUMULATION_STEPS = 1
LEARNING_RATE = 3e-5
WANDB_RUN_NAME = "longformer-action-pred-depth"
LOGGING_STEPS = 100
EVAL_STEPS = 500
SAVE_STEPS = 500

# ======================
# 2. åŠ è½½æˆ–é¢„å¤„ç†æ•°æ®é›†
# ======================
from dataset_utils import get_or_create_dataset


# ======================
# 3. Tokenize å‡½æ•°
# ======================
from dataset_utils import tokenize_function


# ======================
# 4. ä¸»è®­ç»ƒæµç¨‹
# ======================
def main():
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,
                                              cache_dir=HF_CACHE_DIR,  # â† å’Œ download_model.py ä¸€è‡´
                                              clean_up_tokenization_spaces=True  # ä¿æŒå½“å‰è¡Œä¸ºï¼ˆæ¸…ç†ç©ºæ ¼ï¼‰
                                              )
    # Step 1: è·å–å¸§çº§æ•°æ®é›†
    ds = get_or_create_dataset(DATA_DIR, CACHE_DIR)

    # Step 2: åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    ds = ds.train_test_split(test_size=0.05, seed=42)
    train_ds = ds["train"]
    eval_ds = ds["test"]
    # from collections import Counter
    # actions = [ex["action"] for ex in train_ds]
    # print("Action distribution:", Counter(actions))

    # Step 3: Tokenize
    # å¦‚æœæ²¡æœ‰äº‹å…ˆä¿å­˜çš„æ•°æ®é›†ï¼Œåˆ™åˆ›å»ºæ•°æ®é›†
    if os.path.exists(os.path.join(OUTPUT_DIR, "tokenized_train")) and os.path.exists(os.path.join(OUTPUT_DIR, "tokenized_eval")):
        print("Loading tokenized datasets from disk directory:", OUTPUT_DIR)
        from datasets import load_from_disk
        tokenized_train = load_from_disk(os.path.join(OUTPUT_DIR, "tokenized_train"))
        tokenized_eval = load_from_disk(os.path.join(OUTPUT_DIR, "tokenized_eval"))
    else:
        print("Creating and tokenizing datasets...")    
        tokenized_train = train_ds.map(
            lambda x: tokenize_function(x, tokenizer, max_length=MAX_LENGTH),
            batched=True,
            remove_columns=["prompt"]
        )
        tokenized_eval = eval_ds.map(
            lambda x: tokenize_function(x, tokenizer, max_length=MAX_LENGTH),
            batched=True,
            remove_columns=["prompt"]
        )

        tokenized_train = tokenized_train.rename_column("action", "labels")
        tokenized_eval = tokenized_eval.rename_column("action", "labels")

        # ä¿å­˜æ•°æ®é›†ä»¥åè®­ç»ƒæ—¶å¯ç›´æ¥åŠ è½½
        tokenized_train.save_to_disk(os.path.join(OUTPUT_DIR, "tokenized_train"))
        tokenized_eval.save_to_disk(os.path.join(OUTPUT_DIR, "tokenized_eval"))

    # Step 4: ç¡®å®šç±»åˆ«æ•°
    num_labels = len(set(train_ds["action"]))
    print(f"Number of action classes: {num_labels}")

    # Step 5: åŠ è½½æ¨¡å‹
    from model_zoo import WeightedSequenceClassifier
    # è®¡ç®—actionæƒé‡
    from sklearn.utils.class_weight import compute_class_weight
    import numpy as np
    import torch
    labels = np.array(train_ds["action"])
    prompts = np.array(train_ds["prompt"])
    # æ‰“å°ä¸€ä¸ªpromptç¤ºä¾‹ï¼š
    print("Example prompt:", prompts[0])
    class_weights = compute_class_weight(
        class_weight="balanced",
        classes=np.unique(labels),
        y=labels
    )
    # class_weights = torch.tensor(class_weights, dtype=torch.float).to("cuda")  # æˆ– "cpu"
    class_weights = torch.tensor(class_weights, dtype=torch.float) # å¤šå¡è®­ç»ƒæ—¶æ”¾åœ¨ Trainer é‡Œå¤„ç†
    print("Class weights:", class_weights)

    model = WeightedSequenceClassifier(
        MODEL_NAME,
        num_labels=num_labels,
        class_weights=class_weights,
        cache_dir=HF_CACHE_DIR,  # â† å’Œ download_model.py ä¸€è‡´
    )

    # model = AutoModelForSequenceClassification.from_pretrained(
    #     MODEL_NAME,
    #     num_labels=num_labels,
    #     problem_type="single_label_classification"
    # )
    
    # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°æ•°é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total params: {total_params / 1e6:.1f}M")
    print(f"Trainable params: {trainable_params / 1e6:.1f}M ({trainable_params == total_params})")

    # Step 6: å®šä¹‰è¯„ä¼°æŒ‡æ ‡
    # def compute_metrics(eval_pred):
    #     preds, labels = eval_pred
    #     preds = np.argmax(preds, axis=1)
    #     return {"accuracy": accuracy_score(labels, preds)}
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        preds = np.argmax(logits, axis=1)
        
        # Overall
        acc = accuracy_score(labels, preds)
        
        # Per-class recall (critical for rare class)
        report = classification_report(labels, preds, output_dict=True, zero_division=0)
        metrics = {"accuracy": acc}
        for i in range(4):
            metrics[f"recall_class_{i}"] = report[str(i)]["recall"]
            metrics[f"f1_class_{i}"] = report[str(i)]["f1-score"]
        
        return metrics

    # Step 7: è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,          # â†â†â† å…³é”®ä¿®æ”¹ï¼šè®¾ç½®å­¦ä¹ ç‡
        warmup_ratio=0.1,
        weight_decay=0.01,
        logging_steps=LOGGING_STEPS,
        eval_strategy="steps",
        eval_steps=EVAL_STEPS,
        save_strategy="steps",
        save_steps=SAVE_STEPS,
        load_best_model_at_end=True,
        metric_for_best_model="eval_f1_class_0",  # â†â†â† å…³é”®ä¿®æ”¹ï¼šä»¥ç½•è§ç±»F1ä¸ºæœ€ä½³æ¨¡å‹é€‰æ‹©æ ‡å‡†
        greater_is_better=True,
        save_total_limit=2,
        report_to="wandb",                 # â†â†â† å…³é”®ï¼šå¯ç”¨ wandb
        run_name=WANDB_RUN_NAME,    # â† å¯é€‰ï¼šç»™å®éªŒå‘½å
        # report_to="none",
        seed=42,
        dataloader_num_workers=4,
        ddp_find_unused_parameters=False,  # â†â†â† æ·»åŠ è¿™ä¸€è¡Œæ¥å…³é—­ddpçš„unused parameteræ£€æŸ¥
    )

    # Step 8: æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # Step 9: åˆ›å»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # Step 10: å¼€å§‹è®­ç»ƒ
    print("ğŸš€ Starting training...")
    # ç»§ç»­ä¹‹å‰çš„è®­ç»ƒï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    # trainer.train(resume_from_checkpoint=True)
    trainer.train()


    # Step 11: ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_model(os.path.join(OUTPUT_DIR, "final"))
    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, "final"))

    print(f"âœ… Training completed! Model saved to {os.path.join(OUTPUT_DIR, 'final')}")


if __name__ == "__main__":
    main()